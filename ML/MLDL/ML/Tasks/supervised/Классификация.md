

## Метрики качества

### Матрица ошибок (confusion matrix)
- мы предсказали _положительную_ метку и _угадали_. Будем относить такие объекты к **true positive** (**TP**) группе. True — потому что предсказали мы правильно, а positive — потому что предсказали положительную метку;
- мы предсказали _положительную_ метку, но _ошиблись_ в своём предсказании — **false positive** (**FP**). False, потому что предсказание было неправильным;
- мы предсказали _отрицательную_ метку и _угадали_ — **true negative** (**TN**);
- и наконец, мы предсказали _отрицательную_ метку, но _ошиблись_ — **false negative** (**FN**).  
Для удобства все эти 4 числа изображают в виде таблицы, которую называют **confusion matrix** (**матрицей ошибок**).
### Бинарный случай
*Accuracy* - доля правильно распознанных классов по всей выборке.
*Error Rate* - доля неправильно распознанных классов по всей выборке = $1 - Accuracy$
$$
Accuracy(y, y^{pred}) = \frac{1}{N} \sum_{i=1}^{N}I[y_i=f(x_i)]
$$
$$
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
$$
$I$ - индикаторная функция, если условие выполняется равна 1, иначе 0

*Precision* - доля правильно угаданных положительных из всех, которые мы предсказываем положительными
$$
Precision = \frac{TP}{TP + FP}
$$
*Recall* - доля правильно угаданных положительных из всех положительных
$$
Recall = \frac{TP}{TP + FN}
$$
*F1* - среднее гармоническое между *Recall* и *Precision*
$$
F1 = \frac{2}{\frac{1}{Precision} + \frac{1}{Recall}}
$$
## Многоклассовый случай

Пусть есть задача классификации из $K$ классов, ее можно рассмотреть как $K$ бинарных классификаций "один против всех". В таком случае можно поступить 2 способами:
- Сначала посчитать матрицу ошибок для каждой из $K$ бинарных классификаций и потом усреднить $TP$ $TN$ $FP$ $FN$ и посчитать $Recall$ $Precision$, метрика $F1$ в данном случае будет называться - **$F1$ с микроусреднением**.
- Посчитать $Recall$ $Precision$ для каждой из $K$ классификаций и усреднить их, посчитать $F1$, такая метрика называется - **$F1$ с макроусреднением**.
