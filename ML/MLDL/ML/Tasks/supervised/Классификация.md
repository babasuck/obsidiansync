

## Метрики качества
### Невероятностная классификация (метки)
#### Матрица ошибок (confusion matrix)
- мы предсказали _положительную_ метку и _угадали_. Будем относить такие объекты к **true positive** (**TP**) группе. True — потому что предсказали мы правильно, а positive — потому что предсказали положительную метку;
- мы предсказали _положительную_ метку, но _ошиблись_ в своём предсказании — **false positive** (**FP**). False, потому что предсказание было неправильным;
- мы предсказали _отрицательную_ метку и _угадали_ — **true negative** (**TN**);
- и наконец, мы предсказали _отрицательную_ метку, но _ошиблись_ — **false negative** (**FN**).  
Для удобства все эти 4 числа изображают в виде таблицы, которую называют **confusion matrix** (**матрицей ошибок**).
#### Бинарный случай
*Accuracy* - доля правильно распознанных классов по всей выборке.
*Error Rate* - доля неправильно распознанных классов по всей выборке = $1 - Accuracy$
$$
Accuracy(y, y^{pred}) = \frac{1}{N} \sum_{i=1}^{N}I[y_i=f(x_i)]
$$
$$
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
$$
$I$ - индикаторная функция, если условие выполняется равна 1, иначе 0

*Precision* - доля правильно угаданных положительных из всех, которые мы предсказываем положительными
$$
Precision = \frac{TP}{TP + FP}
$$
*Recall* - доля правильно угаданных положительных из всех положительных
$$
Recall = \frac{TP}{TP + FN}
$$
*F1* - среднее гармоническое между *Recall* и *Precision*
$$
F1 = \frac{2}{\frac{1}{Precision} + \frac{1}{Recall}}
$$
### Метрики для вероятностных моделей

При вероятностном подохде к решению задачи классификации, классификатор вместо метки предсказывает вероятность принадлежности объекта к положительному классу. Метка присваивается, если значение вероятности выше заранее заданного порога $w_0$. Формально классификатор задается как:
$$
f(x; w; w_0) = I[g(x,w) > w_0]
$$
**AUC ROC**
Введем метрики *TPR (True Positive Rate)* и *FPR (False Positive Rate)*. При уменьшении порога $w_0$ эти метрики будут расти. 
$$
\begin{align}
TPR = \frac{TP}{TP + FN} \\ \\
FPR = \frac{FP}{FP + TN}
\end{align}
$$

**ROC-кривая** - кривая в осях *TPR* и *FPR* построенная варьированьем порога $w_0$.
**AUC**- площадь под этой кривой. У идеального классификатора площадь 1, у случайного 0.5.
**AUC** равен доле пар объектов вида (объект класса 1, объект класса 0), которые алгоритм верно упорядочил, то есть предсказание классификатора на первом объекте больше.
Эту метрику полезно применять, если важны не метки сами по себе, а правильный порядок объектов.
Позволяет подобрать оптимальный порог отсечения.

**PR кривая** 
Это кривая построенная в осях *Precision* *Recall* при варьировании порога отсечения.
Площадь под этой кривой называется **AP (Average Precision)**
$$
\int_0^1{p(r)dr}
$$
#### Многоклассовый случай

Пусть есть задача классификации из $K$ классов, ее можно рассмотреть как $K$ бинарных классификаций "один против всех". В таком случае можно поступить 2 способами:
- Сначала посчитать матрицу ошибок для каждой из $K$ бинарных классификаций и потом усреднить $TP$ $TN$ $FP$ $FN$ и посчитать $Recall$ $Precision$, метрика $F1$ в данном случае будет называться - **$F1$ с микроусреднением**.
- Посчитать $Recall$ $Precision$ для каждой из $K$ классификаций и усреднить их, посчитать $F1$, такая метрика называется - **$F1$ с макроусреднением**.
